{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNf2Hgk/Y1Ve99WoqkW56Tj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A51689/Python-for-DS-1689/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRDUktuM6aA1",
        "outputId": "e78b81c8-c530-485e-e6e8-aae8d44d50a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Regression] LinearRegression -> RMSE 447.174 R2 0.993\n",
            "[Regression] Ridge -> RMSE 448.985 R2 0.993\n",
            "[Regression] Lasso -> RMSE 447.164 R2 0.993\n",
            "[Regression] RandomForestRegressor -> RMSE 0.000 R2 1.000\n",
            "[Regression] ExtraTreesRegressor -> RMSE 0.616 R2 1.000\n",
            "[Regression] GradientBoostingRegressor -> RMSE 185.316 R2 0.999\n",
            "[Regression] KNeighborsRegressor -> RMSE 543.954 R2 0.990\n",
            "[Regression] SVR -> RMSE 4851.152 R2 0.210\n",
            "[Classification] LogisticRegression -> acc 1.000 f1 1.000 auc 1.0\n",
            "[Classification] RandomForestClassifier -> acc 1.000 f1 1.000 auc 1.0\n",
            "[Classification] ExtraTreesClassifier -> acc 1.000 f1 1.000 auc 1.0\n",
            "[Classification] GradientBoostingClassifier -> acc 1.000 f1 1.000 auc 1.0\n",
            "[Classification] KNeighborsClassifier -> acc 1.000 f1 1.000 auc 1.0\n",
            "[Classification] SVC -> acc 1.000 f1 1.000 auc 1.0\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "ml_full_india_pipeline.py\n",
        "Full ML pipeline for the India pesticide toxicity risk dataset.\n",
        "\n",
        "Usage:\n",
        "  python ml_full_india_pipeline.py --data /mnt/data/india_pesticide_toxicity_risk.csv --outdir ./ml_results\n",
        "\n",
        "Arguments:\n",
        "  --data           Path to CSV dataset\n",
        "  --outdir         Output folder (default ./ml_results)\n",
        "  --max-rows       If set, subsample to this many rows (int)\n",
        "  --rf-estimators  Number of trees for RandomForest/ExtraTrees (default 150)\n",
        "  --skip-tsne      If provided, skip t-SNE (speeds things up)\n",
        "\"\"\"\n",
        "import argparse, os, math, warnings, joblib\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor, GradientBoostingClassifier, ExtraTreesRegressor, ExtraTreesClassifier, IsolationForest\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from sklearn.svm import SVR, SVC, OneClassSVM\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def main(args):\n",
        "    OUT = Path(args.outdir); OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Load\n",
        "    df = pd.read_csv(args.data)\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    if args.max_rows and len(df) > args.max_rows:\n",
        "        df = df.sample(n=args.max_rows, random_state=42)\n",
        "\n",
        "    # Targets detection\n",
        "    reg_target = \"use_quantity_kg_ai_modeled\" if \"use_quantity_kg_ai_modeled\" in df.columns else (\"use_quantity_kg_ai\" if \"use_quantity_kg_ai\" in df.columns else None)\n",
        "    clf_target = \"is_hhp_flag\" if \"is_hhp_flag\" in df.columns else (\"hhp_flag\" if \"hhp_flag\" in df.columns else None)\n",
        "\n",
        "    # Features candidates\n",
        "    num_candidates = [\"year\",\"bee_ld50_mg_per_bee\",\"aetl_lethal_doses\"]\n",
        "    num_features = [c for c in num_candidates if c in df.columns]\n",
        "    cat_candidates = [\"admin1\",\"category\",\"active_ingredient\",\"country\",\"iso3\"]\n",
        "    cat_features = [c for c in cat_candidates if c in df.columns]\n",
        "    if \"year\" not in df.columns:\n",
        "        df[\"year\"] = pd.to_datetime(df.get(\"created_at_utc\", pd.NaT), errors=\"coerce\").dt.year.fillna(2005).astype(int)\n",
        "    if \"year\" not in num_features:\n",
        "        num_features = [\"year\"] + num_features\n",
        "\n",
        "    features = [f for f in (num_features + cat_features) if f in df.columns]\n",
        "    X = df[features].copy()\n",
        "\n",
        "    # Preprocessing\n",
        "    num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
        "    cat_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"__missing__\")), (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
        "    pre = ColumnTransformer([(\"num\", num_pipe, num_features), (\"cat\", cat_pipe, cat_features)])\n",
        "\n",
        "    # Regression models\n",
        "    if reg_target:\n",
        "        y = df[reg_target].astype(float).fillna(0.0)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        models_reg = {\n",
        "            \"LinearRegression\": LinearRegression(),\n",
        "            \"Ridge\": Ridge(alpha=1.0),\n",
        "            \"Lasso\": Lasso(alpha=0.001),\n",
        "            \"RandomForestRegressor\": RandomForestRegressor(n_estimators=args.rf_estimators, random_state=42, n_jobs=-1),\n",
        "            \"ExtraTreesRegressor\": ExtraTreesRegressor(n_estimators=args.rf_estimators, random_state=42, n_jobs=-1),\n",
        "            \"GradientBoostingRegressor\": GradientBoostingRegressor(random_state=42),\n",
        "            \"KNeighborsRegressor\": KNeighborsRegressor(n_neighbors=7),\n",
        "            \"SVR\": SVR()\n",
        "        }\n",
        "        metrics_reg = []\n",
        "        for name, model in models_reg.items():\n",
        "            try:\n",
        "                pipe = Pipeline([(\"pre\", pre), (\"model\", model)])\n",
        "                pipe.fit(X_train, y_train)\n",
        "                pred = pipe.predict(X_test)\n",
        "                rmse = math.sqrt(mean_squared_error(y_test, pred))\n",
        "                mae = mean_absolute_error(y_test, pred)\n",
        "                r2 = r2_score(y_test, pred)\n",
        "                metrics_reg.append({\"model\":name, \"rmse\":rmse, \"mae\":mae, \"r2\":r2})\n",
        "                joblib.dump(pipe, OUT/f\"reg_{name}.joblib\")\n",
        "                print(f\"[Regression] {name} -> RMSE {rmse:.3f} R2 {r2:.3f}\")\n",
        "            except Exception as e:\n",
        "                print(f\"[Regression] {name} failed: {e}\")\n",
        "        pd.DataFrame(metrics_reg).to_csv(OUT/\"metrics_regression.csv\", index=False)\n",
        "\n",
        "    # Classification models\n",
        "    if clf_target:\n",
        "        y = df[clf_target].astype(int).fillna(0).astype(int)\n",
        "        if len(np.unique(y)) > 1:\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "            models_clf = {\n",
        "                \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
        "                \"RandomForestClassifier\": RandomForestClassifier(n_estimators=args.rf_estimators, random_state=42, n_jobs=-1),\n",
        "                \"ExtraTreesClassifier\": ExtraTreesClassifier(n_estimators=args.rf_estimators, random_state=42, n_jobs=-1),\n",
        "                \"GradientBoostingClassifier\": GradientBoostingClassifier(random_state=42),\n",
        "                \"KNeighborsClassifier\": KNeighborsClassifier(n_neighbors=7),\n",
        "                \"SVC\": SVC(probability=True)\n",
        "            }\n",
        "            metrics_clf = []\n",
        "            for name, model in models_clf.items():\n",
        "                try:\n",
        "                    pipe = Pipeline([(\"pre\", pre), (\"model\", model)])\n",
        "                    pipe.fit(X_train, y_train)\n",
        "                    pred = pipe.predict(X_test)\n",
        "                    proba = pipe.predict_proba(X_test)[:,1] if hasattr(pipe.named_steps[\"model\"], \"predict_proba\") else None\n",
        "                    acc = accuracy_score(y_test, pred)\n",
        "                    prec = precision_score(y_test, pred, zero_division=0)\n",
        "                    rec = recall_score(y_test, pred, zero_division=0)\n",
        "                    f1 = f1_score(y_test, pred, zero_division=0)\n",
        "                    auc = roc_auc_score(y_test, proba) if proba is not None else float(\"nan\")\n",
        "                    metrics_clf.append({\"model\":name, \"accuracy\":acc, \"precision\":prec, \"recall\":rec, \"f1\":f1, \"roc_auc\":auc})\n",
        "                    joblib.dump(pipe, OUT/f\"clf_{name}.joblib\")\n",
        "                    print(f\"[Classification] {name} -> acc {acc:.3f} f1 {f1:.3f} auc {auc if not np.isnan(auc) else 'NA'}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"[Classification] {name} failed: {e}\")\n",
        "            pd.DataFrame(metrics_clf).to_csv(OUT/\"metrics_classification.csv\", index=False)\n",
        "        else:\n",
        "            print(\"Classification target has a single class; skipping classification.\")\n",
        "\n",
        "    # Unsupervised: PCA/t-SNE/clustering/anomaly detection\n",
        "    X_trans = pre.fit_transform(X)\n",
        "    if hasattr(X_trans, \"toarray\"):\n",
        "        X_arr = X_trans.toarray()\n",
        "    else:\n",
        "        X_arr = X_trans\n",
        "\n",
        "    # PCA 2D & explained variance\n",
        "    pca2 = PCA(n_components=2, random_state=42).fit_transform(X_arr)\n",
        "    pd.DataFrame(pca2, columns=[\"pc1\",\"pc2\"]).to_csv(OUT/\"pca_2d.csv\", index=False)\n",
        "    pca10 = PCA(n_components=min(10, X_arr.shape[1]-1)).fit(X_arr)\n",
        "    pd.DataFrame({\"component\": list(range(1, pca10.n_components_+1)), \"explained_variance_ratio\": pca10.explained_variance_ratio_}).to_csv(OUT/\"pca_explained_variance.csv\", index=False)\n",
        "\n",
        "    if not args.skip_tsne:\n",
        "        idx = np.random.RandomState(42).choice(X_arr.shape[0], size=min(5000, X_arr.shape[0]), replace=False)\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=30, learning_rate=\"auto\", init=\"pca\", n_iter=800)\n",
        "        X_tsne = tsne.fit_transform(X_arr[idx])\n",
        "        pd.DataFrame(X_tsne, columns=[\"tsne1\",\"tsne2\"]).to_csv(OUT/\"tsne_2d_subsample.csv\", index=False)\n",
        "\n",
        "    # Clustering: KMeans, Agglomerative, DBSCAN, GMM\n",
        "    clust_results = []\n",
        "    for k in [3,4,6]:\n",
        "        try:\n",
        "            km = KMeans(n_clusters=k, random_state=42, n_init=\"auto\").fit(X_arr)\n",
        "            sil = silhouette_score(X_arr, km.labels_) if len(np.unique(km.labels_))>1 else float(\"nan\")\n",
        "            clust_results.append({\"algorithm\":\"KMeans\",\"k\":k,\"silhouette\":sil})\n",
        "        except Exception as e:\n",
        "            clust_results.append({\"algorithm\":\"KMeans\",\"k\":k,\"error\":str(e)})\n",
        "    for k in [3,4,6]:\n",
        "        try:\n",
        "            ag = AgglomerativeClustering(n_clusters=k).fit(X_arr)\n",
        "            sil = silhouette_score(X_arr, ag.labels_) if len(np.unique(ag.labels_))>1 else float(\"nan\")\n",
        "            clust_results.append({\"algorithm\":\"Agglomerative\",\"k\":k,\"silhouette\":sil})\n",
        "        except Exception as e:\n",
        "            clust_results.append({\"algorithm\":\"Agglomerative\",\"k\":k,\"error\":str(e)})\n",
        "    for eps in [0.5, 1.0]:\n",
        "        try:\n",
        "            db = DBSCAN(eps=eps, min_samples=10).fit(X_arr)\n",
        "            labels = db.labels_\n",
        "            nclusters = len([l for l in np.unique(labels) if l!=-1])\n",
        "            sil = silhouette_score(X_arr[labels!=-1], labels[labels!=-1]) if nclusters>1 else float(\"nan\")\n",
        "            clust_results.append({\"algorithm\":\"DBSCAN\",\"eps\":eps,\"nclusters\":nclusters,\"silhouette\":sil})\n",
        "        except Exception as e:\n",
        "            clust_results.append({\"algorithm\":\"DBSCAN\",\"eps\":eps,\"error\":str(e)})\n",
        "    for k in [3,4,6]:\n",
        "        try:\n",
        "            gm = GaussianMixture(n_components=k, random_state=42).fit(X_arr)\n",
        "            labels = gm.predict(X_arr)\n",
        "            sil = silhouette_score(X_arr, labels) if len(np.unique(labels))>1 else float(\"nan\")\n",
        "            clust_results.append({\"algorithm\":\"GMM\",\"k\":k,\"silhouette\":sil})\n",
        "        except Exception as e:\n",
        "            clust_results.append({\"algorithm\":\"GMM\",\"k\":k,\"error\":str(e)})\n",
        "\n",
        "    pd.DataFrame(clust_results).to_csv(OUT/\"metrics_clustering.csv\", index=False)\n",
        "\n",
        "    # Anomaly detection: IsolationForest, OneClassSVM (sampled)\n",
        "    iso = IsolationForest(n_estimators=200, contamination=0.01, random_state=42).fit(X_arr)\n",
        "    iso_labels = iso.predict(X_arr)\n",
        "    iso_rate = float((iso_labels==-1).mean())\n",
        "    try:\n",
        "        ocs = OneClassSVM(gamma=\"scale\", nu=0.05).fit(X_arr[:min(10000, X_arr.shape[0])])\n",
        "        oc_labels = ocs.predict(X_arr[:min(10000, X_arr.shape[0])])\n",
        "        oc_rate = float((oc_labels==-1).mean())\n",
        "    except Exception as e:\n",
        "        oc_rate = None\n",
        "    pd.DataFrame([{\"algorithm\":\"IsolationForest\",\"anomaly_rate\":iso_rate},{\"algorithm\":\"OneClassSVM_sample\",\"anomaly_rate\":oc_rate}]).to_csv(OUT/\"anomaly_rates.csv\", index=False)\n",
        "\n",
        "    print(\"Full pipeline finished. Check outputs in:\", OUT.resolve())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Create a dummy parser and pass arguments directly\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--data\", required=True)\n",
        "    ap.add_argument(\"--outdir\", default=\"./ml_results\")\n",
        "    ap.add_argument(\"--max-rows\", type=int, default=None)\n",
        "    ap.add_argument(\"--rf-estimators\", type=int, default=150)\n",
        "    ap.add_argument(\"--skip-tsne\", action=\"store_true\")\n",
        "    # Pass the arguments as a list to parse_args\n",
        "    args = ap.parse_args([\"--data\", \"/content/india_pesticide_toxicity_risk.zip\"])\n",
        "    main(args)"
      ]
    }
  ]
}